{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - Cumulative Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this cumulative lab, we'll use everything we've learned so far to build a model that can classify a text document as one of many possible classes!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "- Practice cleaning and exploring a text dataset with NLTK and base Python\n",
    "- Practice using scikit-learn vectorizers for text preprocessing\n",
    "- Tune a modeling process through exploration and model evaluation\n",
    "- Observe some techniques for feature engineering\n",
    "- Interpret the result of a final ML model that classifies text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task: Complete an End-to-End ML Process with the Newsgroups Dataset\n",
    "\n",
    "<a title=\"Bundesarchiv, B 145 Bild-F077948-0006 / Engelbert Reineke / CC-BY-SA 3.0, CC BY-SA 3.0 DE &lt;https://creativecommons.org/licenses/by-sa/3.0/de/deed.en&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Bundesarchiv_B_145_Bild-F077948-0006,_Jugend-Computerschule_mit_IBM-PC.jpg\"><img width=\"512\" alt=\"Bundesarchiv B 145 Bild-F077948-0006, Jugend-Computerschule mit IBM-PC\" src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Bundesarchiv_B_145_Bild-F077948-0006%2C_Jugend-Computerschule_mit_IBM-PC.jpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "The ***Newsgroups Dataset*** is a collection of [newsgroup](https://en.wikipedia.org/wiki/Usenet_newsgroup) posts originally collected around 1995. While the backend code implementation is fairly different, you can think of them as like the Reddit posts of 1995, where a \"category\" in this dataset is like a subreddit.\n",
    "\n",
    "The task is to try to identify the category where a post was published, based on the text content of the post.\n",
    "\n",
    "### Data Understanding\n",
    "\n",
    "#### Data Source\n",
    "\n",
    "Part of what you are practicing here is using the `sklearn.datasets` submodule, which you have seen before (e.g. the Iris Dataset, the Wine Dataset). You can see a full list of available dataset loaders [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets).\n",
    "\n",
    "In this case we will be using the `fetch_20newsgroups` function ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)). An important thing to note is that because this is text data, scikit-learn actually downloads a set of documents to the computer you are using to complete this lab, rather than just loading data into memory in Python.\n",
    "\n",
    "#### Features\n",
    "\n",
    "Prior to preprocessing, every row in the dataset only contains one feature: a string containing the full text of the newsgroup post. We will perform preprocessing to create additional features.\n",
    "\n",
    "#### Target\n",
    "\n",
    "As you might have guessed based on the function name, there are 20 categories in the full dataset. Here is a list of all the possible classes:\n",
    "\n",
    "<img src='classes.png'>\n",
    "\n",
    "This full dataset is quite large. To save us from extremely long runtimes, we'll work with only a subset of the classes. For this lab, we'll work with the following five:\n",
    "\n",
    "* `'comp.windows.x'`\n",
    "* `'rec.sport.hockey'`\n",
    "* `'misc.forsale'`\n",
    "* `'sci.crypt'`\n",
    "* `'talk.politics.misc'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "#### 1. Load the Data\n",
    "\n",
    "Use pandas and `sklearn.datasets` to load the train and test data into appropriate data structures. Then get a sense of what is in this dataset by visually inspecting some samples.\n",
    "\n",
    "#### 2. Perform Data Cleaning and Exploratory Data Analysis with `nltk`\n",
    "\n",
    "Standardize the case of the data and use a tokenizer to convert the full posts into lists of individual words. Then compare the raw word frequency distributions of each category.\n",
    "\n",
    "#### 3. Build and Evaluate a Baseline Model with `TfidfVectorizer` and `MultinomialNB`\n",
    "\n",
    "Ultimately all data must be in numeric form in order to be able to fit a scikit-learn model. So we'll use a tool from `sklearn.feature_extraction.text` to convert all data into a vectorized format.\n",
    "\n",
    "Initially we'll keep all of the default parameters for both the vectorizer and the model, in order to develop a baseline score.\n",
    "\n",
    "#### 4. Iteratively Perform and Evaluate Preprocessing and Feature Engineering Techniques\n",
    "\n",
    "Here you will investigate three techniques, to determine whether they should be part of our final modeling process:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Using custom tokens\n",
    "3. Domain-specific feature engineering\n",
    "4. Increasing `max_features`\n",
    "\n",
    "#### 5. Evaluate a Final Model on the Test Set\n",
    "\n",
    "Once you have chosen a final modeling process, fit it on the full training data and evaluate it on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "In the cell below, create the variables `newsgroups_train` and `newsgroups_test` by calling the `fetch_20newsgroups` function twice.\n",
    "\n",
    "For the train set, specify `subset=\"train\"`. For the test set, specify `subset=\"test\"`.\n",
    "\n",
    "Additionally, pass in `remove=('headers', 'footers', 'quotes')` in both function calls, in order to automatically remove some metadata that can lead to overfitting.\n",
    "\n",
    "Recall that we are loading only five categories, out of the full 20. So, pass in `categories=categories` both times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'comp.windows.x',\n",
    "    'rec.sport.hockey',\n",
    "    'misc.forsale',\n",
    "    'sci.crypt',\n",
    "    'talk.politics.misc'\n",
    "]\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset= 'train',\n",
    "    remove= ('headers', 'footers', 'quotes'),\n",
    "    categories= categories\n",
    ")\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    categories=categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the returned objects is a dictionary-like `Bunch` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "type(newsgroups_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing to know is that the `.data` attribute will extract the feature values, and the `.target` attribute will extract the target values. So, for example, the train features (`X_train`) are located in `newsgroups_train.data`, whereas the train targets (`y_train`) are located in `newsgroups_train.target`.\n",
    "\n",
    "In the cell below, create `X_train`, `X_test`, `y_train`, `y_test` based on `newsgroups_train` and `newsgroups_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 400)\n",
    "pd.set_option('use_mathjax', False)\n",
    "\n",
    "# Extract values from Bunch objects\n",
    "X_train = pd.DataFrame(newsgroups_train.data, columns=[\"text\"])\n",
    "X_test = pd.DataFrame(newsgroups_test.data, columns=[\"text\"])\n",
    "y_train = pd.Series(newsgroups_train.data, name=\"category\")\n",
    "y_test = pd.Series(newsgroups_test.data, name=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check that your variables have the correct shape below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# X_train and X_test both have 1 column (text)\n",
    "assert X_train.shape[1] == X_test.shape[1] and X_train.shape[1] == 1\n",
    "\n",
    "# y_train and y_test are 1-dimensional (target value only)\n",
    "assert len(y_train.shape) == len(y_test.shape) and len(y_train.shape) == 1\n",
    "\n",
    "# X_train and y_train have the same number of rows\n",
    "assert X_train.shape[0] == y_train.shape[0] and X_train.shape[0] == 2838\n",
    "\n",
    "# X_test and y_test have the same number of rows\n",
    "assert X_test.shape[0] == y_test.shape[0] and X_test.shape[0] == 1890"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's look at some basic attributes of the dataset.\n",
    "\n",
    "#### Distribution of Target\n",
    "\n",
    "We know that there are five categories represented. How many are there of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run this cell without changes\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_target_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_train\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[1;32m----> 4\u001b[0m train_target_counts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [newsgroups_train\u001b[38;5;241m.\u001b[39mtarget_names[val] \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m train_target_counts\u001b[38;5;241m.\u001b[39mindex]\n\u001b[0;32m      5\u001b[0m train_target_counts\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m train_target_counts\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget value\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run this cell without changes\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_target_counts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(y_train\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[1;32m----> 4\u001b[0m train_target_counts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mnewsgroups_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m train_target_counts\u001b[38;5;241m.\u001b[39mindex]\n\u001b[0;32m      5\u001b[0m train_target_counts\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m train_target_counts\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget value\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "train_target_counts = pd.DataFrame(y_train.value_counts())\n",
    "train_target_counts[\"label\"] = [newsgroups_train.target_names[val] for val in train_target_counts.index]\n",
    "train_target_counts.columns = [\"count\", \"target name\"]\n",
    "train_target_counts.index.name = \"target value\"\n",
    "train_target_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, the category \"comp.windows.x\" has the label of `0` in our dataset, and there are 593 text samples in that category within our training data.\n",
    "\n",
    "We also note that our target distribution looks reasonably balanced. Now let's look at the features.\n",
    "\n",
    "#### Visually Inspecting Features\n",
    "\n",
    "Run the cell below to view some examples of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_27dde_row0_col0, #T_27dde_row0_col1, #T_27dde_row1_col0, #T_27dde_row1_col1, #T_27dde_row2_col0, #T_27dde_row2_col1, #T_27dde_row3_col0, #T_27dde_row3_col1, #T_27dde_row4_col0, #T_27dde_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_27dde\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_27dde_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_27dde_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_27dde_level0_row0\" class=\"row_heading level0 row0\" >1300</th>\n",
       "      <td id=\"T_27dde_row0_col0\" class=\"data row0 col0\" >\n",
       "\n",
       "\n",
       "   Ncd has an excellent document titled \"Host Loading Considerations in the X \n",
       "  environment\". I received my copy by emailing support@ncd.com. This may\n",
       "  help out.</td>\n",
       "      <td id=\"T_27dde_row0_col1\" class=\"data row0 col1\" >\n",
       "\n",
       "\n",
       "   Ncd has an excellent document titled \"Host Loading Considerations in the X \n",
       "  environment\". I received my copy by emailing support@ncd.com. This may\n",
       "  help out.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27dde_level0_row1\" class=\"row_heading level0 row1\" >1758</th>\n",
       "      <td id=\"T_27dde_row1_col0\" class=\"data row1 col0\" >\n",
       "\n",
       " \n",
       "           You don't have to.  *It*  believes in YOU.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        Well, looking at our new government pals, I'm inclined to\n",
       "        agree.  I don't much believe in our money, either. :)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    Oh, ho HO!   If only you knew!  :)\n",
       "\n",
       "    Yup, I'm DEFINITELY checking out foreign currency, thanks to\n",
       "    to this newsgroup.  It sure doesn't take much thinking to realize\n",
       "    what direction the U.S. is headed.\n",
       "\n",
       "\n",
       "</td>\n",
       "      <td id=\"T_27dde_row1_col1\" class=\"data row1 col1\" >\n",
       "\n",
       " \n",
       "           You don't have to.  *It*  believes in YOU.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        Well, looking at our new government pals, I'm inclined to\n",
       "        agree.  I don't much believe in our money, either. :)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    Oh, ho HO!   If only you knew!  :)\n",
       "\n",
       "    Yup, I'm DEFINITELY checking out foreign currency, thanks to\n",
       "    to this newsgroup.  It sure doesn't take much thinking to realize\n",
       "    what direction the U.S. is headed.\n",
       "\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27dde_level0_row2\" class=\"row_heading level0 row2\" >2558</th>\n",
       "      <td id=\"T_27dde_row2_col0\" class=\"data row2 col0\" >Miscellaneous comics for sale. I really would like\n",
       "to get rid of these for lack of space. Buyer pays\n",
       "shipping, and all offers considered. OH, and the\n",
       "first purchase over $20 in any of my posts\n",
       "gets a free Maxx #1/2 coupon, or a trashed copy\n",
       "of Amazing spidey #300. Here goes...\n",
       "\n",
       "\n",
       "Deathlok         #1           $3.00\n",
       "                 2-17         $1.75 each\n",
       "                 Annual #1     2.50\n",
       "                 Special #1    2.00\n",
       "\n",
       "Sleepwalker      1,2,6,8,9,13  7.00 (set) or 1.25\n",
       "                                             each\n",
       "\n",
       "\n",
       "Next Men         #1           $3.00\n",
       "Ray              #1            1.00\n",
       "Deathstroke      5,6           1.75 each\n",
       "Darkhawk         13            1.25\n",
       "New warrior's    18            1.00\n",
       "Fantasti Four    358           2.50\n",
       "Moon Knight      35,36         1.75 each\n",
       "Hulk             386-388       1.50 each\n",
       "\n",
       "Punisher W.Z.    1             2.50\n",
       "Cage             1             1.50\n",
       "X-force          1             2.00\n",
       "Silver Sable     1             2.00\n",
       "X-calibur        26,27,48,49   1.50 each\n",
       "\n",
       "\n",
       "Hearts of Darkness             5.00\n",
       "Infinity Guantlet     1-4      2.50 each\n",
       "Batman v. Pred.       1,3      2.00 each\n",
       " \"   \"  \"  (deluxe)   1        5.00\n",
       "\n",
       "Guardians of the\n",
       "Galaxy                1       3.00\n",
       "Spider-man 2099       1-3     5.00 (set)\n",
       "Spec. spider-man      189     3.00 (special hologram)\n",
       "\n",
       "Let me know if you'd like to buy anything. My\n",
       "address is U38134@uicvm.uic.edu</td>\n",
       "      <td id=\"T_27dde_row2_col1\" class=\"data row2 col1\" >Miscellaneous comics for sale. I really would like\n",
       "to get rid of these for lack of space. Buyer pays\n",
       "shipping, and all offers considered. OH, and the\n",
       "first purchase over $20 in any of my posts\n",
       "gets a free Maxx #1/2 coupon, or a trashed copy\n",
       "of Amazing spidey #300. Here goes...\n",
       "\n",
       "\n",
       "Deathlok         #1           $3.00\n",
       "                 2-17         $1.75 each\n",
       "                 Annual #1     2.50\n",
       "                 Special #1    2.00\n",
       "\n",
       "Sleepwalker      1,2,6,8,9,13  7.00 (set) or 1.25\n",
       "                                             each\n",
       "\n",
       "\n",
       "Next Men         #1           $3.00\n",
       "Ray              #1            1.00\n",
       "Deathstroke      5,6           1.75 each\n",
       "Darkhawk         13            1.25\n",
       "New warrior's    18            1.00\n",
       "Fantasti Four    358           2.50\n",
       "Moon Knight      35,36         1.75 each\n",
       "Hulk             386-388       1.50 each\n",
       "\n",
       "Punisher W.Z.    1             2.50\n",
       "Cage             1             1.50\n",
       "X-force          1             2.00\n",
       "Silver Sable     1             2.00\n",
       "X-calibur        26,27,48,49   1.50 each\n",
       "\n",
       "\n",
       "Hearts of Darkness             5.00\n",
       "Infinity Guantlet     1-4      2.50 each\n",
       "Batman v. Pred.       1,3      2.00 each\n",
       " \"   \"  \"  (deluxe)   1        5.00\n",
       "\n",
       "Guardians of the\n",
       "Galaxy                1       3.00\n",
       "Spider-man 2099       1-3     5.00 (set)\n",
       "Spec. spider-man      189     3.00 (special hologram)\n",
       "\n",
       "Let me know if you'd like to buy anything. My\n",
       "address is U38134@uicvm.uic.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27dde_level0_row3\" class=\"row_heading level0 row3\" >2267</th>\n",
       "      <td id=\"T_27dde_row3_col0\" class=\"data row3 col0\" >\n",
       "\n",
       "My vote goes to Andy Moog 1st, Belfour 2nd, Vanbiesbrouck 3rd\n",
       "\n",
       "The Bruin's are hot at just the right time !!!!!\n",
       "\n",
       "\n",
       "rich beskosty</td>\n",
       "      <td id=\"T_27dde_row3_col1\" class=\"data row3 col1\" >\n",
       "\n",
       "My vote goes to Andy Moog 1st, Belfour 2nd, Vanbiesbrouck 3rd\n",
       "\n",
       "The Bruin's are hot at just the right time !!!!!\n",
       "\n",
       "\n",
       "rich beskosty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27dde_level0_row4\" class=\"row_heading level0 row4\" >1043</th>\n",
       "      <td id=\"T_27dde_row4_col0\" class=\"data row4 col0\" >\n",
       "\n",
       "Yes, \"Clipper\" is a trademark of Intergraph.  Its the RISC chip used\n",
       "in some of thier workstations. \n",
       "\n",
       "I wonder what Intergraph is going to do to this infringement on thier\n",
       "name sake?\n",
       "</td>\n",
       "      <td id=\"T_27dde_row4_col1\" class=\"data row4 col1\" >\n",
       "\n",
       "Yes, \"Clipper\" is a trademark of Intergraph.  Its the RISC chip used\n",
       "in some of thier workstations. \n",
       "\n",
       "I wonder what Intergraph is going to do to this infringement on thier\n",
       "name sake?\n",
       "</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24102423e50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Sample 5 records and display full text of each\n",
    "train_sample = X_train.sample(5, random_state=22)\n",
    "train_sample[\"label\"] = [y_train[val] for val in train_sample.index]\n",
    "train_sample.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order, we have:\n",
    "\n",
    "* An example of `comp.windows.x`, talking about \"host loading considerations\"\n",
    "* An example of `talk.politics.misc`, talking about government and currency\n",
    "* An example of `misc.forsale`, talking about a list of comics for sale\n",
    "* An example of `rec.sport.hockey`, talking about hockey players and the Bruins\n",
    "* An example of `sci.crypt`, talking about a microprocessor\n",
    "\n",
    "We appear to have loaded the data correctly, so let's move on and perform some cleaning and additional exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Data Cleaning and Exploratory Data Analysis with `nltk`\n",
    "\n",
    "Prior to any exploratory analysis, we'll complete two common data cleaning tasks for text data: standardizing case and tokenizing.\n",
    "\n",
    "### Standardizing Case\n",
    "\n",
    "In an NLP modeling process, sometimes we will want to preserve the original case of words (i.e. to treat `\"It\"` and `\"it\"` as different words, and sometimes we will want to standardize case (i.e. to treat `\"It\"` and `\"it\"` as the same word).\n",
    "\n",
    "To figure out what we want to do, let's look at the first sample from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n   Ncd has an excellent document titled \"Host Loading Considerations in the X \\n  environment\". I received my copy by emailing support@ncd.com. This may\\n  help out.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "windows_sample = train_sample.iloc[0][\"text\"]\n",
    "windows_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have two references to the company Network Computing Devices, or NCD. At the beginning, the poster refers to it as `\"Ncd\"`. Then later refers to `\"support@ncd.com\"`. It seems reasonable to assume that both of these should be treated as references to the same word instead of treating `\"Ncd\"` and `\"ncd\"` as two totally separate things. So let's standardize the case of all letters in this dataset.\n",
    "\n",
    "The typical way to standardize case is to make everything lowercase. While it's possible to do this after tokenizing, it's easier and faster to do it first.\n",
    "\n",
    "For a single sample, we can just use the built-in Python `.lower()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n   ncd has an excellent document titled \"host loading considerations in the x \\n  environment\". i received my copy by emailing support@ncd.com. this may\\n  help out.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "windows_sample.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standarizing Case in the Full Dataset\n",
    "\n",
    "To access this method in pandas, you use `.str.lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e984b_row0_col0, #T_e984b_row0_col1, #T_e984b_row1_col0, #T_e984b_row1_col1, #T_e984b_row2_col0, #T_e984b_row2_col1, #T_e984b_row3_col0, #T_e984b_row3_col1, #T_e984b_row4_col0, #T_e984b_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e984b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e984b_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_e984b_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e984b_level0_row0\" class=\"row_heading level0 row0\" >1300</th>\n",
       "      <td id=\"T_e984b_row0_col0\" class=\"data row0 col0\" >\n",
       "\n",
       "\n",
       "   ncd has an excellent document titled \"host loading considerations in the x \n",
       "  environment\". i received my copy by emailing support@ncd.com. this may\n",
       "  help out.</td>\n",
       "      <td id=\"T_e984b_row0_col1\" class=\"data row0 col1\" >\n",
       "\n",
       "\n",
       "   Ncd has an excellent document titled \"Host Loading Considerations in the X \n",
       "  environment\". I received my copy by emailing support@ncd.com. This may\n",
       "  help out.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e984b_level0_row1\" class=\"row_heading level0 row1\" >1758</th>\n",
       "      <td id=\"T_e984b_row1_col0\" class=\"data row1 col0\" >\n",
       "\n",
       " \n",
       "           you don't have to.  *it*  believes in you.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        well, looking at our new government pals, i'm inclined to\n",
       "        agree.  i don't much believe in our money, either. :)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    oh, ho ho!   if only you knew!  :)\n",
       "\n",
       "    yup, i'm definitely checking out foreign currency, thanks to\n",
       "    to this newsgroup.  it sure doesn't take much thinking to realize\n",
       "    what direction the u.s. is headed.\n",
       "\n",
       "\n",
       "</td>\n",
       "      <td id=\"T_e984b_row1_col1\" class=\"data row1 col1\" >\n",
       "\n",
       " \n",
       "           You don't have to.  *It*  believes in YOU.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        Well, looking at our new government pals, I'm inclined to\n",
       "        agree.  I don't much believe in our money, either. :)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    Oh, ho HO!   If only you knew!  :)\n",
       "\n",
       "    Yup, I'm DEFINITELY checking out foreign currency, thanks to\n",
       "    to this newsgroup.  It sure doesn't take much thinking to realize\n",
       "    what direction the U.S. is headed.\n",
       "\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e984b_level0_row2\" class=\"row_heading level0 row2\" >2558</th>\n",
       "      <td id=\"T_e984b_row2_col0\" class=\"data row2 col0\" >miscellaneous comics for sale. i really would like\n",
       "to get rid of these for lack of space. buyer pays\n",
       "shipping, and all offers considered. oh, and the\n",
       "first purchase over $20 in any of my posts\n",
       "gets a free maxx #1/2 coupon, or a trashed copy\n",
       "of amazing spidey #300. here goes...\n",
       "\n",
       "\n",
       "deathlok         #1           $3.00\n",
       "                 2-17         $1.75 each\n",
       "                 annual #1     2.50\n",
       "                 special #1    2.00\n",
       "\n",
       "sleepwalker      1,2,6,8,9,13  7.00 (set) or 1.25\n",
       "                                             each\n",
       "\n",
       "\n",
       "next men         #1           $3.00\n",
       "ray              #1            1.00\n",
       "deathstroke      5,6           1.75 each\n",
       "darkhawk         13            1.25\n",
       "new warrior's    18            1.00\n",
       "fantasti four    358           2.50\n",
       "moon knight      35,36         1.75 each\n",
       "hulk             386-388       1.50 each\n",
       "\n",
       "punisher w.z.    1             2.50\n",
       "cage             1             1.50\n",
       "x-force          1             2.00\n",
       "silver sable     1             2.00\n",
       "x-calibur        26,27,48,49   1.50 each\n",
       "\n",
       "\n",
       "hearts of darkness             5.00\n",
       "infinity guantlet     1-4      2.50 each\n",
       "batman v. pred.       1,3      2.00 each\n",
       " \"   \"  \"  (deluxe)   1        5.00\n",
       "\n",
       "guardians of the\n",
       "galaxy                1       3.00\n",
       "spider-man 2099       1-3     5.00 (set)\n",
       "spec. spider-man      189     3.00 (special hologram)\n",
       "\n",
       "let me know if you'd like to buy anything. my\n",
       "address is u38134@uicvm.uic.edu</td>\n",
       "      <td id=\"T_e984b_row2_col1\" class=\"data row2 col1\" >Miscellaneous comics for sale. I really would like\n",
       "to get rid of these for lack of space. Buyer pays\n",
       "shipping, and all offers considered. OH, and the\n",
       "first purchase over $20 in any of my posts\n",
       "gets a free Maxx #1/2 coupon, or a trashed copy\n",
       "of Amazing spidey #300. Here goes...\n",
       "\n",
       "\n",
       "Deathlok         #1           $3.00\n",
       "                 2-17         $1.75 each\n",
       "                 Annual #1     2.50\n",
       "                 Special #1    2.00\n",
       "\n",
       "Sleepwalker      1,2,6,8,9,13  7.00 (set) or 1.25\n",
       "                                             each\n",
       "\n",
       "\n",
       "Next Men         #1           $3.00\n",
       "Ray              #1            1.00\n",
       "Deathstroke      5,6           1.75 each\n",
       "Darkhawk         13            1.25\n",
       "New warrior's    18            1.00\n",
       "Fantasti Four    358           2.50\n",
       "Moon Knight      35,36         1.75 each\n",
       "Hulk             386-388       1.50 each\n",
       "\n",
       "Punisher W.Z.    1             2.50\n",
       "Cage             1             1.50\n",
       "X-force          1             2.00\n",
       "Silver Sable     1             2.00\n",
       "X-calibur        26,27,48,49   1.50 each\n",
       "\n",
       "\n",
       "Hearts of Darkness             5.00\n",
       "Infinity Guantlet     1-4      2.50 each\n",
       "Batman v. Pred.       1,3      2.00 each\n",
       " \"   \"  \"  (deluxe)   1        5.00\n",
       "\n",
       "Guardians of the\n",
       "Galaxy                1       3.00\n",
       "Spider-man 2099       1-3     5.00 (set)\n",
       "Spec. spider-man      189     3.00 (special hologram)\n",
       "\n",
       "Let me know if you'd like to buy anything. My\n",
       "address is U38134@uicvm.uic.edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e984b_level0_row3\" class=\"row_heading level0 row3\" >2267</th>\n",
       "      <td id=\"T_e984b_row3_col0\" class=\"data row3 col0\" >\n",
       "\n",
       "my vote goes to andy moog 1st, belfour 2nd, vanbiesbrouck 3rd\n",
       "\n",
       "the bruin's are hot at just the right time !!!!!\n",
       "\n",
       "\n",
       "rich beskosty</td>\n",
       "      <td id=\"T_e984b_row3_col1\" class=\"data row3 col1\" >\n",
       "\n",
       "My vote goes to Andy Moog 1st, Belfour 2nd, Vanbiesbrouck 3rd\n",
       "\n",
       "The Bruin's are hot at just the right time !!!!!\n",
       "\n",
       "\n",
       "rich beskosty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e984b_level0_row4\" class=\"row_heading level0 row4\" >1043</th>\n",
       "      <td id=\"T_e984b_row4_col0\" class=\"data row4 col0\" >\n",
       "\n",
       "yes, \"clipper\" is a trademark of intergraph.  its the risc chip used\n",
       "in some of thier workstations. \n",
       "\n",
       "i wonder what intergraph is going to do to this infringement on thier\n",
       "name sake?\n",
       "</td>\n",
       "      <td id=\"T_e984b_row4_col1\" class=\"data row4 col1\" >\n",
       "\n",
       "Yes, \"Clipper\" is a trademark of Intergraph.  Its the RISC chip used\n",
       "in some of thier workstations. \n",
       "\n",
       "I wonder what Intergraph is going to do to this infringement on thier\n",
       "name sake?\n",
       "</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24102dbf310>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Transform sample data to lowercase\n",
    "train_sample[\"text\"] = train_sample[\"text\"].str.lower()\n",
    "# Display full text\n",
    "train_sample.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, perform the same operation on the full `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Transform text in X_train to lowercase\n",
    "X_train['text'] = X_train['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check your work by looking at an example and making sure the text is lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i have a problem where an athena strip chart widget is not calling it's\\nget value function.  i am pretty sure this is happening because i am\\nnot using xtappmainloop, but am dealing with events via sockets.  (ya ya).\\n\\nanyway, i want to cause a timeout so that the strip chart widget(s) will\\ncall their get value callback.  or if someone knows another fast way around\\nthis (or any way for that matter) let me know.  i cannot (or i don't think)\\ncall the xtngetvalue callback myself because i don't have the value for\\nthe third parameter of the get value proc (xtpointer call_data).  \\n\\nin other words, i want to force a strip chart widget to update itself.\\n\\nany ideas anyone?  \\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "X_train.iloc[100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Now that the case is consistent it's time to convert each document from a single long string into a set of tokens.\n",
    "\n",
    "Let's look more closely at the second example from our training data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n \\n           you don't have to.  *it*  believes in you.\\n\\n\\n\\n\\n        well, looking at our new government pals, i'm inclined to\\n        agree.  i don't much believe in our money, either. :)\\n\\n\\n\\n\\n    oh, ho ho!   if only you knew!  :)\\n\\n    yup, i'm definitely checking out foreign currency, thanks to\\n    to this newsgroup.  it sure doesn't take much thinking to realize\\n    what direction the u.s. is headed.\\n\\n\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "politics_sample = train_sample.iloc[1][\"text\"]\n",
    "politics_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we split this into tokens just by using the built-in Python `.split` string method, we would have a lot of punctuation attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " \"don't\",\n",
       " 'have',\n",
       " 'to.',\n",
       " '*it*',\n",
       " 'believes',\n",
       " 'in',\n",
       " 'you.',\n",
       " 'well,',\n",
       " 'looking']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "politics_sample.split()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Punctuation being attached to words is a problem because we probably want to treat `you` and `you.` as two instances of the same token, not two different tokens.)\n",
    "\n",
    "Let's use the default token pattern that scikit-learn uses in its vectorizers. The RegEx looks like this:\n",
    "\n",
    "```\n",
    "(?u)\\b\\w\\w+\\b\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "1. `(?u)`: use full unicode string matching\n",
    "2. `\\b`: find a word boundary (a word boundary has length 0, and represents the location between non-word characters and word characters)\n",
    "3. `\\w\\w+`: find 2 or more word characters (all letters, numbers, and underscores are word characters)\n",
    "4. `\\b`: find another word boundary\n",
    "\n",
    "In other words, we are looking for tokens that consist of two or more consecutive word characters, which include letters, numbers, and underscores.\n",
    "\n",
    "We'll use the `RegexpTokenizer` from NLTK to create these tokens, initially just transforming the politics sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'don', 'have', 'to', 'it', 'believes', 'in', 'you', 'well', 'looking']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "basic_token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(basic_token_pattern)\n",
    "tokenizer.tokenize(politics_sample)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the Full Dataset\n",
    "\n",
    "The way to tokenize all values in a column of a pandas dataframe is to use `.apply` and pass in `tokenizer.tokenize`.\n",
    "\n",
    "For example, with the sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eda61_row0_col0, #T_eda61_row0_col1, #T_eda61_row0_col2, #T_eda61_row1_col0, #T_eda61_row1_col1, #T_eda61_row1_col2, #T_eda61_row2_col0, #T_eda61_row2_col1, #T_eda61_row2_col2, #T_eda61_row3_col0, #T_eda61_row3_col1, #T_eda61_row3_col2, #T_eda61_row4_col0, #T_eda61_row4_col1, #T_eda61_row4_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eda61\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eda61_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_eda61_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "      <th id=\"T_eda61_level0_col2\" class=\"col_heading level0 col2\" >text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eda61_level0_row0\" class=\"row_heading level0 row0\" >1300</th>\n",
       "      <td id=\"T_eda61_row0_col0\" class=\"data row0 col0\" >\n",
       "\n",
       "\n",
       "   ncd has an excellent document titled \"host loading considerations in the x \n",
       "  environment\". i received my copy by emailing support@ncd.com. this may\n",
       "  help out.</td>\n",
       "      <td id=\"T_eda61_row0_col1\" class=\"data row0 col1\" >\n",
       "\n",
       "\n",
       "   Ncd has an excellent document titled \"Host Loading Considerations in the X \n",
       "  environment\". I received my copy by emailing support@ncd.com. This may\n",
       "  help out.</td>\n",
       "      <td id=\"T_eda61_row0_col2\" class=\"data row0 col2\" >['ncd', 'has', 'an', 'excellent', 'document', 'titled', 'host', 'loading', 'considerations', 'in', 'the', 'environment', 'received', 'my', 'copy', 'by', 'emailing', 'support', 'ncd', 'com', 'this', 'may', 'help', 'out']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eda61_level0_row1\" class=\"row_heading level0 row1\" >1758</th>\n",
       "      <td id=\"T_eda61_row1_col0\" class=\"data row1 col0\" >\n",
       "\n",
       " \n",
       "           you don't have to.  *it*  believes in you.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        well, looking at our new government pals, i'm inclined to\n",
       "        agree.  i don't much believe in our money, either. :)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    oh, ho ho!   if only you knew!  :)\n",
       "\n",
       "    yup, i'm definitely checking out foreign currency, thanks to\n",
       "    to this newsgroup.  it sure doesn't take much thinking to realize\n",
       "    what direction the u.s. is headed.\n",
       "\n",
       "\n",
       "</td>\n",
       "      <td id=\"T_eda61_row1_col1\" class=\"data row1 col1\" >\n",
       "\n",
       " \n",
       "           You don't have to.  *It*  believes in YOU.\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "        Well, looking at our new government pals, I'm inclined to\n",
       "        agree.  I don't much believe in our money, either. :)\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "    Oh, ho HO!   If only you knew!  :)\n",
       "\n",
       "    Yup, I'm DEFINITELY checking out foreign currency, thanks to\n",
       "    to this newsgroup.  It sure doesn't take much thinking to realize\n",
       "    what direction the U.S. is headed.\n",
       "\n",
       "\n",
       "</td>\n",
       "      <td id=\"T_eda61_row1_col2\" class=\"data row1 col2\" >['you', 'don', 'have', 'to', 'it', 'believes', 'in', 'you', 'well', 'looking', 'at', 'our', 'new', 'government', 'pals', 'inclined', 'to', 'agree', 'don', 'much', 'believe', 'in', 'our', 'money', 'either', 'oh', 'ho', 'ho', 'if', 'only', 'you', 'knew', 'yup', 'definitely', 'checking', 'out', 'foreign', 'currency', 'thanks', 'to', 'to', 'this', 'newsgroup', 'it', 'sure', 'doesn', 'take', 'much', 'thinking', 'to', 'realize', 'what', 'direction', 'the', 'is', 'headed']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eda61_level0_row2\" class=\"row_heading level0 row2\" >2558</th>\n",
       "      <td id=\"T_eda61_row2_col0\" class=\"data row2 col0\" >miscellaneous comics for sale. i really would like\n",
       "to get rid of these for lack of space. buyer pays\n",
       "shipping, and all offers considered. oh, and the\n",
       "first purchase over $20 in any of my posts\n",
       "gets a free maxx #1/2 coupon, or a trashed copy\n",
       "of amazing spidey #300. here goes...\n",
       "\n",
       "\n",
       "deathlok         #1           $3.00\n",
       "                 2-17         $1.75 each\n",
       "                 annual #1     2.50\n",
       "                 special #1    2.00\n",
       "\n",
       "sleepwalker      1,2,6,8,9,13  7.00 (set) or 1.25\n",
       "                                             each\n",
       "\n",
       "\n",
       "next men         #1           $3.00\n",
       "ray              #1            1.00\n",
       "deathstroke      5,6           1.75 each\n",
       "darkhawk         13            1.25\n",
       "new warrior's    18            1.00\n",
       "fantasti four    358           2.50\n",
       "moon knight      35,36         1.75 each\n",
       "hulk             386-388       1.50 each\n",
       "\n",
       "punisher w.z.    1             2.50\n",
       "cage             1             1.50\n",
       "x-force          1             2.00\n",
       "silver sable     1             2.00\n",
       "x-calibur        26,27,48,49   1.50 each\n",
       "\n",
       "\n",
       "hearts of darkness             5.00\n",
       "infinity guantlet     1-4      2.50 each\n",
       "batman v. pred.       1,3      2.00 each\n",
       " \"   \"  \"  (deluxe)   1        5.00\n",
       "\n",
       "guardians of the\n",
       "galaxy                1       3.00\n",
       "spider-man 2099       1-3     5.00 (set)\n",
       "spec. spider-man      189     3.00 (special hologram)\n",
       "\n",
       "let me know if you'd like to buy anything. my\n",
       "address is u38134@uicvm.uic.edu</td>\n",
       "      <td id=\"T_eda61_row2_col1\" class=\"data row2 col1\" >Miscellaneous comics for sale. I really would like\n",
       "to get rid of these for lack of space. Buyer pays\n",
       "shipping, and all offers considered. OH, and the\n",
       "first purchase over $20 in any of my posts\n",
       "gets a free Maxx #1/2 coupon, or a trashed copy\n",
       "of Amazing spidey #300. Here goes...\n",
       "\n",
       "\n",
       "Deathlok         #1           $3.00\n",
       "                 2-17         $1.75 each\n",
       "                 Annual #1     2.50\n",
       "                 Special #1    2.00\n",
       "\n",
       "Sleepwalker      1,2,6,8,9,13  7.00 (set) or 1.25\n",
       "                                             each\n",
       "\n",
       "\n",
       "Next Men         #1           $3.00\n",
       "Ray              #1            1.00\n",
       "Deathstroke      5,6           1.75 each\n",
       "Darkhawk         13            1.25\n",
       "New warrior's    18            1.00\n",
       "Fantasti Four    358           2.50\n",
       "Moon Knight      35,36         1.75 each\n",
       "Hulk             386-388       1.50 each\n",
       "\n",
       "Punisher W.Z.    1             2.50\n",
       "Cage             1             1.50\n",
       "X-force          1             2.00\n",
       "Silver Sable     1             2.00\n",
       "X-calibur        26,27,48,49   1.50 each\n",
       "\n",
       "\n",
       "Hearts of Darkness             5.00\n",
       "Infinity Guantlet     1-4      2.50 each\n",
       "Batman v. Pred.       1,3      2.00 each\n",
       " \"   \"  \"  (deluxe)   1        5.00\n",
       "\n",
       "Guardians of the\n",
       "Galaxy                1       3.00\n",
       "Spider-man 2099       1-3     5.00 (set)\n",
       "Spec. spider-man      189     3.00 (special hologram)\n",
       "\n",
       "Let me know if you'd like to buy anything. My\n",
       "address is U38134@uicvm.uic.edu</td>\n",
       "      <td id=\"T_eda61_row2_col2\" class=\"data row2 col2\" >['miscellaneous', 'comics', 'for', 'sale', 'really', 'would', 'like', 'to', 'get', 'rid', 'of', 'these', 'for', 'lack', 'of', 'space', 'buyer', 'pays', 'shipping', 'and', 'all', 'offers', 'considered', 'oh', 'and', 'the', 'first', 'purchase', 'over', '20', 'in', 'any', 'of', 'my', 'posts', 'gets', 'free', 'maxx', 'coupon', 'or', 'trashed', 'copy', 'of', 'amazing', 'spidey', '300', 'here', 'goes', 'deathlok', '00', '17', '75', 'each', 'annual', '50', 'special', '00', 'sleepwalker', '13', '00', 'set', 'or', '25', 'each', 'next', 'men', '00', 'ray', '00', 'deathstroke', '75', 'each', 'darkhawk', '13', '25', 'new', 'warrior', '18', '00', 'fantasti', 'four', '358', '50', 'moon', 'knight', '35', '36', '75', 'each', 'hulk', '386', '388', '50', 'each', 'punisher', '50', 'cage', '50', 'force', '00', 'silver', 'sable', '00', 'calibur', '26', '27', '48', '49', '50', 'each', 'hearts', 'of', 'darkness', '00', 'infinity', 'guantlet', '50', 'each', 'batman', 'pred', '00', 'each', 'deluxe', '00', 'guardians', 'of', 'the', 'galaxy', '00', 'spider', 'man', '2099', '00', 'set', 'spec', 'spider', 'man', '189', '00', 'special', 'hologram', 'let', 'me', 'know', 'if', 'you', 'like', 'to', 'buy', 'anything', 'my', 'address', 'is', 'u38134', 'uicvm', 'uic', 'edu']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eda61_level0_row3\" class=\"row_heading level0 row3\" >2267</th>\n",
       "      <td id=\"T_eda61_row3_col0\" class=\"data row3 col0\" >\n",
       "\n",
       "my vote goes to andy moog 1st, belfour 2nd, vanbiesbrouck 3rd\n",
       "\n",
       "the bruin's are hot at just the right time !!!!!\n",
       "\n",
       "\n",
       "rich beskosty</td>\n",
       "      <td id=\"T_eda61_row3_col1\" class=\"data row3 col1\" >\n",
       "\n",
       "My vote goes to Andy Moog 1st, Belfour 2nd, Vanbiesbrouck 3rd\n",
       "\n",
       "The Bruin's are hot at just the right time !!!!!\n",
       "\n",
       "\n",
       "rich beskosty</td>\n",
       "      <td id=\"T_eda61_row3_col2\" class=\"data row3 col2\" >['my', 'vote', 'goes', 'to', 'andy', 'moog', '1st', 'belfour', '2nd', 'vanbiesbrouck', '3rd', 'the', 'bruin', 'are', 'hot', 'at', 'just', 'the', 'right', 'time', 'rich', 'beskosty']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eda61_level0_row4\" class=\"row_heading level0 row4\" >1043</th>\n",
       "      <td id=\"T_eda61_row4_col0\" class=\"data row4 col0\" >\n",
       "\n",
       "yes, \"clipper\" is a trademark of intergraph.  its the risc chip used\n",
       "in some of thier workstations. \n",
       "\n",
       "i wonder what intergraph is going to do to this infringement on thier\n",
       "name sake?\n",
       "</td>\n",
       "      <td id=\"T_eda61_row4_col1\" class=\"data row4 col1\" >\n",
       "\n",
       "Yes, \"Clipper\" is a trademark of Intergraph.  Its the RISC chip used\n",
       "in some of thier workstations. \n",
       "\n",
       "I wonder what Intergraph is going to do to this infringement on thier\n",
       "name sake?\n",
       "</td>\n",
       "      <td id=\"T_eda61_row4_col2\" class=\"data row4 col2\" >['yes', 'clipper', 'is', 'trademark', 'of', 'intergraph', 'its', 'the', 'risc', 'chip', 'used', 'in', 'some', 'of', 'thier', 'workstations', 'wonder', 'what', 'intergraph', 'is', 'going', 'to', 'do', 'to', 'this', 'infringement', 'on', 'thier', 'name', 'sake']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x241048eb5b0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Create new column with tokenized data\n",
    "train_sample[\"text_tokenized\"] = train_sample[\"text\"].apply(tokenizer.tokenize)\n",
    "# Display full text\n",
    "train_sample.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, apply the same operation on `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Create column text_tokenized on X_train\n",
    "X_train[\"text_tokenized\"] = X_train[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect your work below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'problem',\n",
       " 'where',\n",
       " 'an',\n",
       " 'athena',\n",
       " 'strip',\n",
       " 'chart',\n",
       " 'widget',\n",
       " 'is',\n",
       " 'not',\n",
       " 'calling',\n",
       " 'it',\n",
       " 'get',\n",
       " 'value',\n",
       " 'function',\n",
       " 'am',\n",
       " 'pretty',\n",
       " 'sure',\n",
       " 'this',\n",
       " 'is']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "X_train.iloc[100][\"text_tokenized\"][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that we have removed all single-letter words, so instead of `\"have\", \"a\", \"problem\"`, the sample now shows just `\"have\", \"problem\"`. If we wanted to include single-letter words, we could use the token pattern `(?u)\\b\\w+\\b` instead.)\n",
    "\n",
    "Now that our data is cleaned up (case standardized and tokenized), we can perform some EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Frequency Distributions\n",
    "\n",
    "Recall that a frequency distribution is a data structure that contains pieces of data as well as the count of how frequently they appear. In this case, the pieces of data we'll be looking at are tokens (words).\n",
    "\n",
    "In the past we have built a frequency distribution \"by hand\" using built-in Python data structures. Here we'll use another handy tool from NLTK called `FreqDist` ([documentation here](http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist)). `FreqDist` allows us to pass in a single list of words, and it produces a dictionary-like output of those words and their frequencies.\n",
    "\n",
    "For example, this creates a frequency distribution of the example shown above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'is': 2, 'have': 1, 'problem': 1, 'where': 1, 'an': 1, 'athena': 1, 'strip': 1, 'chart': 1, 'widget': 1, 'not': 1, ...})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "from nltk import FreqDist\n",
    "\n",
    "example_freq_dist = FreqDist(X_train.iloc[100][\"text_tokenized\"][:20])\n",
    "example_freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then can use Matplotlib to visualize the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "visualize_top_10(example_freq_dist, \"Top 10 Word Frequency for Example Tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the chart above is a bit artificial, since this sample only included 20 tokens. But essentially this is saying that the token with the highest frequency in our example is `\"is\"`, which occurred twice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Frequency Distribution for the Full Dataset\n",
    "\n",
    "Let's do that for the full `X_train`.\n",
    "\n",
    "First, we need a list of all of the words in the `text_tokenized` column. We could do this manually by looping over the rows, but fortunately pandas has a handy method called `.explode()` ([documentation here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.explode.html#pandas.Series.explode)) that does exactly this.\n",
    "\n",
    "Here is an example applying that to the sample dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300             ncd\n",
       "1300             has\n",
       "1300              an\n",
       "1300       excellent\n",
       "1300        document\n",
       "            ...     \n",
       "1043    infringement\n",
       "1043              on\n",
       "1043           thier\n",
       "1043            name\n",
       "1043            sake\n",
       "Name: text_tokenized, Length: 289, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "train_sample[\"text_tokenized\"].explode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the top 10 words from the sample dataframe like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "sample_freq_dist = FreqDist(train_sample[\"text_tokenized\"].explode())\n",
    "visualize_top_10(sample_freq_dist, \"Top 10 Word Frequency for 5 Samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `\"00\"` and `\"50\"` are both in the top 10 tokens, due to many prices appearing in the `misc.forsale` example.\n",
    "\n",
    "In the cell below, complete the same process for the full `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Create a frequency distribution for X_train\n",
    "train_freq_dist = FreqDist(X_train[\"text_tokenized\"].explode())\n",
    "\n",
    "# Plot the top 10 tokens\n",
    "visualize_top_10(train_freq_dist, \"Top 10 Word Frequency for Full X_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great, we have a general sense of the word frequencies in our dataset!\n",
    "\n",
    "We can also subdivide this by category, to see if it makes a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m         ax\u001b[38;5;241m.\u001b[39mtick_params(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n\u001b[0;32m     53\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m setup_five_subplots()\n\u001b[1;32m---> 54\u001b[0m \u001b[43mplot_distribution_of_column_by_category\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_tokenized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Frequencies for All Tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36mplot_distribution_of_column_by_category\u001b[1;34m(column, axes, title)\u001b[0m\n\u001b[0;32m     37\u001b[0m freq_dist \u001b[38;5;241m=\u001b[39m FreqDist(all_words)\n\u001b[0;32m     38\u001b[0m top_10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfreq_dist\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m)))\n\u001b[1;32m---> 39\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtop_10\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     40\u001b[0m counts \u001b[38;5;241m=\u001b[39m top_10[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Set up plot\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Add in labels for filtering (we won't pass them in to the model)\n",
    "X_train[\"label\"] = [y_train[val] for val in X_train.index]\n",
    "\n",
    "def setup_five_subplots():\n",
    "    \"\"\"\n",
    "    It's hard to make an odd number of graphs pretty with just nrows\n",
    "    and ncols, so we make a custom grid. See example for more details:\n",
    "    https://matplotlib.org/stable/gallery/subplots_axes_and_figures/gridspec_multicolumn.html\n",
    "\n",
    "    We want the graphs to look like this:\n",
    "     [ ] [ ] [ ]\n",
    "       [ ] [ ]\n",
    "\n",
    "    So we make a 2x6 grid with 5 graphs arranged on it. 3 in the\n",
    "    top row, 2 in the second row\n",
    "\n",
    "      0 1 2 3 4 5\n",
    "    0|[|]|[|]|[|]|\n",
    "    1| |[|]|[|]| |\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(15,9))\n",
    "    fig.set_tight_layout(True)\n",
    "    gs = fig.add_gridspec(2, 6)\n",
    "    ax1 = fig.add_subplot(gs[0, :2]) # row 0, cols 0-1\n",
    "    ax2 = fig.add_subplot(gs[0, 2:4])# row 0, cols 2-3\n",
    "    ax3 = fig.add_subplot(gs[0, 4:]) # row 0, cols 4-5\n",
    "    ax4 = fig.add_subplot(gs[1, 1:3])# row 1, cols 1-2\n",
    "    ax5 = fig.add_subplot(gs[1, 3:5])# row 1, cols 3-4\n",
    "    return fig, [ax1, ax2, ax3, ax4, ax5]\n",
    "\n",
    "def plot_distribution_of_column_by_category(column, axes, title=\"Word Frequency for\"):\n",
    "    for index, category in enumerate(newsgroups_train.target_names):\n",
    "        # Calculate frequency distribution for this subset\n",
    "        all_words = X_train[X_train[\"label\"] == index][column].explode()\n",
    "        freq_dist = FreqDist(all_words)\n",
    "        top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "        tokens = top_10[0]\n",
    "        counts = top_10[1]\n",
    "\n",
    "        # Set up plot\n",
    "        ax = axes[index]\n",
    "        ax.bar(tokens, counts)\n",
    "\n",
    "        # Customize plot appearance\n",
    "        ax.set_title(f\"{title} {category}\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "\n",
    "fig, axes = setup_five_subplots()\n",
    "plot_distribution_of_column_by_category(\"text_tokenized\", axes)\n",
    "fig.suptitle(\"Word Frequencies for All Tokens\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these were unlabeled, would you be able to figure out which one matched with which category?\n",
    "\n",
    "Well, `misc.forsale` still has a number (`\"00\"`) as one of its top tokens, so you might be able to figure out that one, but it seems very difficult to distinguish the others; every single category has `\"the\"` as the most common token, and every category except for `misc.forsale` has `\"to\"` as the second most common token. \n",
    "\n",
    "After building our baseline model, we'll use this information to inform our next preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build and Evaluate a Baseline Model with `TfidfVectorizer` and `MultinomialNB`\n",
    "\n",
    "Let's start modeling by building a model that basically only has access to the information in the plots above. So, using the default token pattern to split the full text into tokens, and using a limited vocabulary.\n",
    "\n",
    "To give the model a little bit more information with those same features, we'll use a `TfidfVectorizer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)) so that it counts not only the term frequency (`tf`) within a single document, it also includes the inverse document frequency (`idf`) — how rare the term is.\n",
    "\n",
    "In the cell below, import the vectorizer, instantiate a vectorizer object, and fit it on `X_train[\"text\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>of</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.322609</td>\n",
       "      <td>0.077590</td>\n",
       "      <td>0.304553</td>\n",
       "      <td>0.238740</td>\n",
       "      <td>0.203477</td>\n",
       "      <td>0.331334</td>\n",
       "      <td>0.290966</td>\n",
       "      <td>0.278467</td>\n",
       "      <td>0.334292</td>\n",
       "      <td>0.561259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090518</td>\n",
       "      <td>0.097966</td>\n",
       "      <td>0.096133</td>\n",
       "      <td>0.100479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092966</td>\n",
       "      <td>0.104965</td>\n",
       "      <td>0.937591</td>\n",
       "      <td>0.253249</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.187451</td>\n",
       "      <td>0.367889</td>\n",
       "      <td>0.192259</td>\n",
       "      <td>0.196634</td>\n",
       "      <td>0.355768</td>\n",
       "      <td>0.401688</td>\n",
       "      <td>0.448504</td>\n",
       "      <td>0.484575</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.468758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355598</td>\n",
       "      <td>0.520342</td>\n",
       "      <td>0.152052</td>\n",
       "      <td>0.206330</td>\n",
       "      <td>0.077654</td>\n",
       "      <td>0.462422</td>\n",
       "      <td>0.312257</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328237</td>\n",
       "      <td>0.322097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261785</td>\n",
       "      <td>0.848518</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>0.135376</td>\n",
       "      <td>0.097676</td>\n",
       "      <td>0.095849</td>\n",
       "      <td>0.450819</td>\n",
       "      <td>0.256154</td>\n",
       "      <td>0.370765</td>\n",
       "      <td>0.418620</td>\n",
       "      <td>0.350558</td>\n",
       "      <td>0.505001</td>\n",
       "      <td>0.058880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>0.296277</td>\n",
       "      <td>0.192393</td>\n",
       "      <td>0.755176</td>\n",
       "      <td>0.328880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.121716</td>\n",
       "      <td>0.274852</td>\n",
       "      <td>0.306886</td>\n",
       "      <td>0.110522</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.794502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.228205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105889</td>\n",
       "      <td>0.103908</td>\n",
       "      <td>0.325815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502423</td>\n",
       "      <td>0.567271</td>\n",
       "      <td>0.168903</td>\n",
       "      <td>0.091243</td>\n",
       "      <td>0.510644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2838 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           and       for        in        is        it        of      that  \\\n",
       "0     0.322609  0.077590  0.304553  0.238740  0.203477  0.331334  0.290966   \n",
       "1     0.090518  0.097966  0.096133  0.100479  0.000000  0.092966  0.104965   \n",
       "2     0.173200  0.187451  0.367889  0.192259  0.196634  0.355768  0.401688   \n",
       "3     0.468758  0.000000  0.355598  0.520342  0.152052  0.206330  0.077654   \n",
       "4     0.000000  0.328237  0.322097  0.000000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2833  0.135376  0.097676  0.095849  0.450819  0.256154  0.370765  0.418620   \n",
       "2834  0.296277  0.192393  0.755176  0.328880  0.000000  0.121716  0.274852   \n",
       "2835  0.489400  0.794502  0.000000  0.000000  0.277808  0.000000  0.000000   \n",
       "2836  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2837  0.000000  0.105889  0.103908  0.325815  0.000000  0.502423  0.567271   \n",
       "\n",
       "           the        to       you  \n",
       "0     0.278467  0.334292  0.561259  \n",
       "1     0.937591  0.253249  0.000000  \n",
       "2     0.448504  0.484575  0.000000  \n",
       "3     0.462422  0.312257  0.000000  \n",
       "4     0.261785  0.848518  0.000000  \n",
       "...        ...       ...       ...  \n",
       "2833  0.350558  0.505001  0.058880  \n",
       "2834  0.306886  0.110522  0.000000  \n",
       "2835  0.000000  0.228205  0.000000  \n",
       "2836  0.000000  0.000000  0.000000  \n",
       "2837  0.168903  0.091243  0.510644  \n",
       "\n",
       "[2838 rows x 10 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the relevant vectorizer class\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(max_features=10)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of your vectorized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# We should still have the same number of rows\n",
    "assert X_train_vectorized.shape[0] == X_train.shape[0]\n",
    "\n",
    "# The vectorized version should have 10 columns, since we set\n",
    "# max_features=10\n",
    "assert X_train_vectorized.shape[1] == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have preprocessed data, fit and evaluate a multinomial Naive Bayes classifier ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)) using `cross_val_score` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01232394, 0.01232394, 0.01232394, 0.01234568, 0.01058201])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Import relevant class and function\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate a MultinomialNB classifier\n",
    "baseline_model = MultinomialNB()\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "baseline_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "baseline_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well is this model performing? Well, recall the class balance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.011980\n",
       "\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.003171\n",
       "\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.001057\n",
       "10 month old stereo system for sale. Luxman R-351 receiver, Onkyo TA-RW404\\ntape deck, and Polk Monitor M4.6 book shelf speakers are for sale. Receiver\\nhas 5 year warranty, and all equipment is in excellent condition. Paid $950\\nfor the system and willing to consider the best offer. Will sell seperate\\npieces also if desired. Please send best offer to suraj@cs.jhu.edu.\\n\\nSpeakers: Polk Monitor M4.6 bookshelf speakers\\n\\t  Paid $250 pair. Willing to consider best offer.\\n\\nReceiver: Luxman R-351 receiver with 5 year (yes 5 years) warranty.\\n\\t  Paid $475. Willing to consider best offer.\\n\\t  Full remote, 2 pairs of speaker connections,\\n\\t  60 watts per channel, but drives like a 150 watts per channel\\n\\t  Has all the standard features, and more.\\n\\nTape Deck: Onkyo TA-RW404 tape deck\\n\\t   Paid $275. Willing to consider best offer.\\n\\t   Dual cassette, Dolby B, C, and HX Pro.\\n\\t   Input level control for recording, auto reverse both sides.\\n           Has all standard features.\\n\\nSend E-mail with best offer to suraj@cs.jhu.edu                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.000705\n",
       "Hi\\n\\nI am trying to implement a pointer feature in Xlib\\n\\nI have multiple windows and all can take input and \\nshow output simultaneously on all other displays\\n\\nI want to implement a pointer feature \\n\\nI would like to get the pointer to come up on all windows once \\nI choose pointer in the menu and every one should be able\\nto see it\\n\\nCan you give me some hints as to how I should proceed \\nI am new to Xlib\\n\\n\\n\\nreplies will be greatly appreciated\\n\\nThank you                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.000705\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "\\n\\tThis appears to be generic calling upon the name of the anti-christ.\\nJust for the hell of it, let's destroy this remark. Let us imagine that\\nthe executive branch actually could extract keys from the escrow houses\\nwithout anyone knowing, or telling. Now what? Dick has 80 bits of data.\\nWhat the hell's he gonna do with it?\\n\\n\\t1) Trot around to the telco and say 'we'd like an unauthorised\\ndecrypting tap'. Uh huh.\\n\\t2) Break in to watergate and install his own tap (so his people still\\ndo have to break in, neat, huh?) record some noise, then get the Executive\\nBranch Phone Decryption Box (huh? they've got one? Goodness, wait 'til the\\nwashington post gets hold of this) and decrypt the noise.\\n\\t3) More likely, stare at the key, and say 'Oh, hell it's not\\nworth all this bloody hassle'\\n\\n\\tTruth is, even granted *lots* of covert power on the part of\\nthe Executive Branch, this system is *more* difficult to tap with than\\nPOTS gear. The fact that it is easier to tap than some hypothetical\\nsystem neither you nor I am going to place on our phones is neither\\nhere nor there.\\n\\n\\tThe only rational concerns I am seeing raised are:\\n\\n\\ta) is the key really just chopped in half, and not some XOR\\narrangement? That is, has some egregious technical error been built\\nin to the plan?\\n\\tb) is this is the first step toward strict regulation of strong\\nencryption?\\n\\n\\n\\n\\tThis is b), of course. I suspect not. If the government actually\\nwanted to make such regs, they'd just do it. A few hundred people on Usenet\\nyelling about it wouldn't even slow the machine down.\\n\\n\\tBesides, who is this mysterious 'they' who's going to take away\\nall our rights the instant we let our guard down? Congress? That gang\\nof buffoons can't even balance their checkbooks. The FBI? But.. they\\ndon't make the laws. The NSA? Ditto. The white house? Bill Clinton\\nis probably still looking for the bathroom. It's a big place, after all.\\n\\n\\tAndrew    0.000352\n",
       "I offer $100, shipment at seller's expense, payment as personal check\\nsent by U.S. mail within 24 hours after receiving goods.  I reserve the\\nright to return the goods, at my expense, if I find them to be defective\\nor otherwise unacceptable when I receive them (either the merchandise or\\nthe check would be mailed within 24 hours).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.000352\n",
       "Dbase IV 1.5 for sale, 3.5 inch disks, all registration included (so you\\ncan upgrade to 2.0 if you want), manuals still shrinkwrapped, disks only\\nopened to verify they all work.  Asking $175 or best offer.\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.000352\n",
       "\\tDoug> NNTP-Posting-Host: se05.wg2.waii.com\\n\\n\\tDoug> NNTP-Posting-Host: se05.wg2.waii.com\\n\\tDoug> I  am  having  a  big problem trying to build MIT X11R5 with xlc 1.2.1\\n\\tDoug> (the  one  bundled  with  AIX  3.2.3e).   Its almost the linker is not\\n\\tDoug> working properly with shared libraries.\\n\\n\\tDoug> I've built X11R5 with no problem before .. but now its all  headaches.\\n\\tDoug> For example, the xset client complains that libXmu doesnt have a bunch\\n\\tDoug> of Xt routines and shr.o is missing (or  something  like  that).   The\\n\\tDoug> build of libXmu DOES link in libXt  so I am really perplexed  what  is\\n\\tDoug> going on.\\n\\n\\n\\tDoug> ....following up on this, the specific error I get is:\\n\\tDoug> Could not load program ./xset \\n\\tDoug> Symbol XtVaSetValues in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtName in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtWindowOfObject in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtGetConstraintResourceList in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtDisplayToApplicationContext in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtAppSetTypeConverter in ../.././lib/Xmu/libXmu.a is undefined\\n\\n\\n\\n\\n\\n\\n\\n\\n\\tDoug> Symbol XtScreenDatabase in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtResolvePathname in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtCvtStringToFont in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtCallConverter in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Symbol XtDisplayStringConversionWarning in ../.././lib/Xmu/libXmu.a is undefined\\n\\tDoug> Could not load library libXmu.a[shr.o]\\n\\tDoug> Error was: Exec format error\\n\\n\\n....  a  search  on  IBMLINK  revealed that this is similar to IX33890\\n(howervre this was closed USE).                                                                                                                                                    0.000352\n",
       "Clipper might be a good way to cover the use of another layer of\\nencryption.\\n\\nCurrently, when you send an encrypted message, an opponent can usually\\ntell 1) that you are using encryption 2) which encryption method you\\nare using [because that information is usually in the clear].\\n\\nWith clipper, most opponents will only know that you are sending\\nclipper-text, they won't know that your clipper-text is itself\\nencoded.\\n\\nOnly those few opponents who get your clipper-keys will know\\nthat your message is double encrypted.\\n\\n... kind of like a safety deposit box containing a lock box.\\n\\nSo, don't just think of replacements for clipper, also think of front\\nends.\\n\\n- Carl\\n\\n\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.000352\n",
       "Name: category, Length: 2779, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we guessed the plurality class every time (class `2`), we would expect about 21% accuracy. So when this model is getting 37-42% accuracy, that is a clear improvement over just guessing. But with an accuracy below 50%, we still expect the model to guess the wrong class the majority of the time. Let's see if we can improve that with more sophisticated preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Iteratively Perform and Evaluate Preprocessing and Feature Engineering Techniques\n",
    "\n",
    "Now that we have our baseline, the fun part begins. As you've seen throughout this section, preprocessing text data is a bit more challenging that working with more traditional data types because there's no clear-cut answer for exactly what sort of preprocessing we need to do. As we are preprocessing our text data, we need to make some decisions about things such as:\n",
    "\n",
    "* Do we remove stop words or not?\n",
    "* What should be counted as a token? Do we stem or lemmatize our text data, or leave the words as is? Do we want to include non-\"words\" in our tokens?\n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?\n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?\n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?\n",
    "\n",
    "In this lab, we will work through the first four of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "\n",
    "Let's begin with the first question: ***do we remove stopwords or not?*** In general we assume that stopwords do not contain useful information, but that is not always the case. Let's empirically investigate the top word frequencies of each category to see whether removing stopwords helps us to distinguish between the catogories.\n",
    "\n",
    "As-is, recall that the raw word frequency distributions of 4 out of 5 categories look very similar. They start with `the` as the word with by far the highest frequency, then there is a downward slope of other common words, starting with `to`. The `misc.forsale` category looks a little different, but it still has `the` as the top token.\n",
    "\n",
    "If we remove stopwords, how does this change the frequency distributions for each category?\n",
    "\n",
    "#### Stopwords List\n",
    "\n",
    "Once again, NLTK has a useful tool for this task. You can just import a list of standard stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize that list as well.\n",
    "\n",
    "Let's say that we want to keep the word `\"for\"` in our final vocabulary, since it appears disproportionately often in the `misc.forsale` category. The code below removes that from the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list length: 179\n",
      "List length after removing 'for': 178\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Original list length:\", len(stopwords_list))\n",
    "stopwords_list.pop(stopwords_list.index(\"for\"))\n",
    "print(\"List length after removing 'for':\", len(stopwords_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, write a function `remove_stopwords` that takes in a list-like collection of strings (tokens) and returns only those that are not in the list of stopwords. (Use the `stopwords_list` in the global scope, so that we can later use `.apply` with this function.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, return a list where the tokens\n",
    "    that are also present in stopwords_list have been\n",
    "    removed\n",
    "    \"\"\"\n",
    "    stopwords_removed = [token for token in token_list if token not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out on one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length with stopwords: 110\n",
      "Length without stopwords: 65\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "tokens_example = X_train.iloc[100][\"text_tokenized\"]\n",
    "print(\"Length with stopwords:\", len(tokens_example))\n",
    "assert len(tokens_example) == 110\n",
    "\n",
    "tokens_example_without_stopwords = remove_stopwords(tokens_example)\n",
    "print(\"Length without stopwords:\", len(tokens_example_without_stopwords))\n",
    "assert len(tokens_example_without_stopwords) == 65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that ran successfully, go ahead and apply it to the full `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "X_train[\"text_without_stopwords\"] = X_train[\"text_tokenized\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare frequency distributions without stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run this cell without changes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m setup_five_subplots()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_distribution_of_column_by_category\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_without_stopwords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Frequencies without Stopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36mplot_distribution_of_column_by_category\u001b[1;34m(column, axes, title)\u001b[0m\n\u001b[0;32m     37\u001b[0m freq_dist \u001b[38;5;241m=\u001b[39m FreqDist(all_words)\n\u001b[0;32m     38\u001b[0m top_10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfreq_dist\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m)))\n\u001b[1;32m---> 39\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtop_10\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     40\u001b[0m counts \u001b[38;5;241m=\u001b[39m top_10[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Set up plot\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "fig, axes = setup_five_subplots()\n",
    "plot_distribution_of_column_by_category(\"text_without_stopwords\", axes)\n",
    "fig.suptitle(\"Word Frequencies without Stopwords\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this seems to answer our question. The most common words differ significantly between categories now, meaning that hopefully our model will have an easier time distinguishing between them.\n",
    "\n",
    "Let's redo our modeling process, using `stopwords_list` when instantiating the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edu</th>\n",
       "      <th>for</th>\n",
       "      <th>get</th>\n",
       "      <th>key</th>\n",
       "      <th>like</th>\n",
       "      <th>new</th>\n",
       "      <th>one</th>\n",
       "      <th>people</th>\n",
       "      <th>use</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502172</td>\n",
       "      <td>0.864768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.908329</td>\n",
       "      <td>0.418257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575451</td>\n",
       "      <td>0.495478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.442862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867274</td>\n",
       "      <td>0.497831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.695901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.718138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312260</td>\n",
       "      <td>0.537729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.587966</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2838 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           edu       for       get  key      like       new       one  \\\n",
       "0     0.000000  0.599601  0.000000  0.0  0.000000  0.561043  0.000000   \n",
       "1     0.000000  0.502172  0.864768  0.0  0.000000  0.000000  0.000000   \n",
       "2     0.000000  1.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "3     0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "4     0.908329  0.418257  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2833  0.000000  0.575451  0.495478  0.0  0.476688  0.000000  0.000000   \n",
       "2834  0.000000  0.867274  0.497831  0.0  0.000000  0.000000  0.000000   \n",
       "2835  0.000000  0.695901  0.000000  0.0  0.000000  0.000000  0.718138   \n",
       "2836  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "2837  0.000000  0.312260  0.537729  0.0  0.517336  0.000000  0.000000   \n",
       "\n",
       "        people       use     would  \n",
       "0     0.570709  0.000000  0.000000  \n",
       "1     0.000000  0.000000  0.000000  \n",
       "2     0.000000  0.000000  0.000000  \n",
       "3     0.525951  0.000000  0.850515  \n",
       "4     0.000000  0.000000  0.000000  \n",
       "...        ...       ...       ...  \n",
       "2833  0.000000  0.000000  0.442862  \n",
       "2834  0.000000  0.000000  0.000000  \n",
       "2835  0.000000  0.000000  0.000000  \n",
       "2836  0.000000  0.000000  0.000000  \n",
       "2837  0.000000  0.587966  0.000000  \n",
       "\n",
       "[2838 rows x 10 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stopwords_list\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01232394, 0.01232394, 0.01232394, 0.01234568, 0.01058201])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "stopwords_removed_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "stopwords_removed_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to our baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:          0.01197990411605435\n",
      "Stopwords removed: 0.01197990411605435\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Baseline:         \", baseline_cv.mean())\n",
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a marginal improvement, but still an improvement. So, to answer ***do we remove stopwords or not:*** yes, let's remove stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Custom Tokens\n",
    "\n",
    "Our next question is ***what should be counted as a token?***\n",
    "\n",
    "Recall that currently we are using the default token pattern, which finds words of two or more characters. What happens if we also *stem* those words, so that `swims` and `swimming` would count as the same token?\n",
    "\n",
    "Here we have provided a custom tokenizing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "def stem_and_tokenize(document):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses `tokenizer` that we created earlier, as well as a new `stemmer` object. See an example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: ['happening', 'because', 'am', 'not', 'using', 'xtappmainloop', 'but', 'am', 'dealing', 'with']\n",
      "Stemmed sample:  ['happen', 'becaus', 'am', 'not', 'use', 'xtappmainloop', 'but', 'am', 'deal', 'with']\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Original sample:\", X_train.iloc[100][\"text_tokenized\"][20:30])\n",
    "print(\"Stemmed sample: \", stem_and_tokenize(X_train.iloc[100][\"text\"])[20:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to stem our stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "stemmed_stopwords = [stemmer.stem(word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, repeat the modeling process from earlier. This time when instantiating the `TfidfVectorizer`, specify:\n",
    "\n",
    "* `max_features=10` (same as previous)\n",
    "* `stop_words=stemmed_stopwords` (modified)\n",
    "* `tokenizer=stem_and_tokenize` (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1047299072.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [79]\u001b[1;36m\u001b[0m\n\u001b[1;33m    X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stemmed_stopwords,\n",
    "    tokenizer=stem_and_tokenize\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Visually inspect the vectorized data\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01232394, 0.01232394, 0.01232394, 0.01234568, 0.01058201])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Evaluate the classifier on X_train_vectorized and y_train\n",
    "stemmed_cv = cross_val_score(baseline_model, X_train_vectorized, y_train)\n",
    "stemmed_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to our previous best modeling process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed: 0.01197990411605435\n",
      "Stemmed:           0.01197990411605435\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Stopwords removed:\", stopwords_removed_cv.mean())\n",
    "print(\"Stemmed:          \", stemmed_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Another improvement, a slightly bigger one than we got when just removing stopwords. So, our best modeling process for now is one where we remove stopwords, use the default token pattern, and stem our tokens with a snowball stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain-Specific Feature Engineering\n",
    "\n",
    "The way to really get the most information out of text data is by adding features beyond just vectorizing the tokens. This code will be completed for you, and it's okay if you don't fully understand everything that is happening, but we hope it helps you brainstorm for future projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Sentences\n",
    "\n",
    "Does the number of sentences in a post differ by category? Let's investigate.\n",
    "\n",
    "Once again, there is a tool from NLTK that helps with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i have a problem where an athena strip chart widget is not calling it's\\nget value function.\",\n",
       " 'i am pretty sure this is happening because i am\\nnot using xtappmainloop, but am dealing with events via sockets.',\n",
       " '(ya ya).',\n",
       " 'anyway, i want to cause a timeout so that the strip chart widget(s) will\\ncall their get value callback.',\n",
       " 'or if someone knows another fast way around\\nthis (or any way for that matter) let me know.',\n",
       " \"i cannot (or i don't think)\\ncall the xtngetvalue callback myself because i don't have the value for\\nthe third parameter of the get value proc (xtpointer call_data).\",\n",
       " 'in other words, i want to force a strip chart widget to update itself.',\n",
       " 'any ideas anyone?']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(X_train.iloc[100][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just take the length of this list to find the number of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "len(sent_tokenize(X_train.iloc[100][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code adds a feature `num_sentences` to `X_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "X_train[\"num_sentences\"] = X_train[\"text\"].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run this cell without changes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m setup_five_subplots()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_distribution_of_column_by_category\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_sentences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNumbers of Sentences for\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributions of Sentence Counts by Category\u001b[39m\u001b[38;5;124m\"\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36mplot_distribution_of_column_by_category\u001b[1;34m(column, axes, title)\u001b[0m\n\u001b[0;32m     37\u001b[0m freq_dist \u001b[38;5;241m=\u001b[39m FreqDist(all_words)\n\u001b[0;32m     38\u001b[0m top_10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfreq_dist\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m)))\n\u001b[1;32m---> 39\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtop_10\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     40\u001b[0m counts \u001b[38;5;241m=\u001b[39m top_10[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Set up plot\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "fig, axes = setup_five_subplots()\n",
    "plot_distribution_of_column_by_category(\"num_sentences\", axes, \"Numbers of Sentences for\")\n",
    "fig.suptitle(\"Distributions of Sentence Counts by Category\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this seem like a useful feature? Maybe. The distributions differ a bit, but it's hard to know if our model will pick up on this information. Let's go ahead and keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contains a Price\n",
    "\n",
    "The idea here is particularly to be able to distinguish the `misc.forsale` category, but it might also help with identifying the others. Let's use RegEx to check if the text contains a price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Define a price as a dollar sign followed by 1-3 numbers,\n",
    "# optional commas or decimals, 1-2 numbers after the decimal\n",
    "# (we're not too worried about accidentally matching malformed prices)\n",
    "price_query = r'\\$(?:\\d{1,3}[,.]?)+(?:\\\\d{1,2})?'\n",
    "\n",
    "X_train[\"contains_price\"] = X_train[\"text\"].str.contains(price_query)\n",
    "\n",
    "fig, axes = setup_five_subplots()\n",
    "plot_distribution_of_column_by_category(\"contains_price\", axes, \"Freqency of Posts Containing Prices for\")\n",
    "fig.suptitle(\"Distributions of Posts Containing Prices by Category\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the `misc.forsale` category looks pretty different from the others. More than half of those posts contain prices, whereas the overwhelming majority of posts in other categories do not contain prices. Let's include this in our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contains an Emoticon\n",
    "\n",
    "This is a bit silly, but we were wondering whether different categories feature different numbers of emoticons.\n",
    "\n",
    "Here we define an emoticon as an ASCII character representing eyes, an optional ASCII character representing a nose, and an ASCII character representing a mouth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "emoticon_query = r'(?:[\\:;X=B][-^]?[)\\]3D([OP/\\\\|])(?:(?=\\s))'\n",
    "\n",
    "X_train[\"contains_emoticon\"] = X_train[\"text\"].str.contains(emoticon_query)\n",
    "\n",
    "fig, axes = setup_five_subplots()\n",
    "plot_distribution_of_column_by_category(\"contains_emoticon\", axes, \"Freqency of Posts Containing Emoticons for\")\n",
    "fig.suptitle(\"Distributions of Posts Containing Emoticons by Category\", fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was a lot less definitive. Emoticons are fairly rare across categories. But, there are some small differences so let's go ahead and keep it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling with Vectorized Features + Engineered Features \n",
    "\n",
    "Let's combine our best vectorizer with these new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10,\n",
    "    stop_words=stemmed_stopwords,\n",
    "    tokenizer=stem_and_tokenize\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Create a full df of vectorized + engineered features\n",
    "X_train_vectorized_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names())\n",
    "preprocessed_X_train = pd.concat([\n",
    "    X_train_vectorized_df, X_train[[\"num_sentences\", \"contains_price\", \"contains_emoticon\"]]\n",
    "], axis=1)\n",
    "preprocessed_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "preprocessed_cv = cross_val_score(baseline_model, preprocessed_X_train, y_train)\n",
    "preprocessed_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Stemmed:           \", stemmed_cv.mean())\n",
    "print(\"Fully preprocessed:\", preprocessed_cv.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, another small improvement! We're still a bit below 50% accuracy, but we're getting improvements every time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing `max_features`\n",
    "\n",
    "Right now we are only allowing the model to look at the tf-idf of the top 10 most frequent tokens. If we allow it to look at all possible tokens, that could lead to high dimensionality issues (especially if we have more rows than columns), but there is a lot of room between 10 and `len(X_train)` features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In other words, setting `max_features` to 2838 would mean an equal number of rows and columns, something that can cause problems for many model algorithms.)\n",
    "\n",
    "Let's try increasing `max_features` from 10 to 200:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=None,\n",
    "    stop_words=stemmed_stopwords,\n",
    "    tokenizer=stem_and_tokenize\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train[\"text\"])\n",
    "\n",
    "# Create a full df of vectorized + engineered features\n",
    "X_train_vectorized_df = pd.DataFrame(X_train_vectorized.toarray(), columns=tfidf.get_feature_names())\n",
    "final_X_train = pd.concat([\n",
    "    X_train_vectorized_df, X_train[[\"num_sentences\", \"contains_price\", \"contains_emoticon\"]]\n",
    "], axis=1)\n",
    "final_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "final_cv = cross_val_score(baseline_model, final_X_train, y_train)\n",
    "final_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Our model was able to learn a lot more with these added features. Let's say this is our final modeling process and move on to a final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate a Final Model on the Test Set\n",
    "\n",
    "Instantiate the model, fit it on the full training set and check the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "final_model = MultinomialNB()\n",
    "\n",
    "final_model.fit(final_X_train, y_train)\n",
    "final_model.score(final_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vectorized version of `X_test`'s text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Note that we just transform, don't fit_transform\n",
    "X_test_vectorized = tfidf.transform(X_test[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering for `X_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "X_test[\"num_sentences\"] = X_test[\"text\"].apply(lambda x: len(sent_tokenize(x)))\n",
    "X_test[\"contains_price\"] = X_test[\"text\"].str.contains(price_query)\n",
    "X_test[\"contains_emoticon\"] = X_test[\"text\"].str.contains(emoticon_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "X_test_vectorized_df = pd.DataFrame(X_test_vectorized.toarray(), columns=tfidf.get_feature_names())\n",
    "final_X_test = pd.concat([\n",
    "    X_test_vectorized_df, X_test[[\"num_sentences\", \"contains_price\", \"contains_emoticon\"]]\n",
    "], axis=1)\n",
    "final_X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "final_model.score(final_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(final_model, final_X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that these are the names associated with the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "target_values_and_names = train_target_counts.drop(\"count\", axis=1)\n",
    "target_values_and_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "\n",
    "Interpret the results seen above. How well did the model do? How does it compare to random guessing? What can you say about the cases that the model was most likely to mislabel? If this were a project and you were describing next steps, what might those be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we used our NLP skills to clean, preprocess, explore, and fit models to text data for classification. This wasn't easy — great job!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
